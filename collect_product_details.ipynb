{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d7622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T12:26:12.649417Z",
     "start_time": "2023-06-20T12:26:12.631760Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6370c3",
   "metadata": {},
   "source": [
    "## All class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a09fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T12:30:22.532931Z",
     "start_time": "2023-06-20T12:30:22.522933Z"
    }
   },
   "outputs": [],
   "source": [
    "class BrowserInitializer:\n",
    "    def __init__(self, no_gui=False, proxy=False):\n",
    "        executable = ''\n",
    "\n",
    "        if platform.system() == 'Windows':\n",
    "            print('Detected OS : Windows')\n",
    "            executable = 'window/chromedriver.exe'\n",
    "        elif platform.system() == 'Linux':\n",
    "            print('Detected OS : Linux')\n",
    "            os.system('chmod +x ./linux/chromedriver')  \n",
    "            executable = './linux/chromedriver'\n",
    "        elif platform.system() == 'Darwin':\n",
    "            print('Detected OS : Mac')\n",
    "            executable = './chromedriver/chromedriver.exe'\n",
    "        else:\n",
    "            raise OSError('Unknown OS Type')\n",
    "\n",
    "        if not osp.exists(executable):\n",
    "            raise FileNotFoundError('Chromedriver file should be placed at {}'.format(executable))\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        if no_gui:\n",
    "            chrome_options.add_argument('--headless')\n",
    "        if proxy:\n",
    "            chrome_options.add_argument(\"--proxy-server={}\".format(proxy))\n",
    "        self.browser = webdriver.Chrome(service=Service(executable), options=chrome_options)\n",
    "\n",
    "        browser_version = 'Failed to detect version'\n",
    "        chromedriver_version = 'Failed to detect version'\n",
    "        major_version_different = False\n",
    "\n",
    "        if 'browserVersion' in self.browser.capabilities:\n",
    "            browser_version = str(self.browser.capabilities['browserVersion'])\n",
    "\n",
    "        if 'chrome' in self.browser.capabilities:\n",
    "            if 'chromedriverVersion' in self.browser.capabilities['chrome']:\n",
    "                chromedriver_version = str(self.browser.capabilities['chrome']['chromedriverVersion']).split(' ')[0]\n",
    "\n",
    "        if browser_version.split('.')[0] != chromedriver_version.split('.')[0]:\n",
    "            major_version_different = True\n",
    "\n",
    "        print('_________________________________')\n",
    "        print('Current web-browser version:\\t{}'.format(browser_version))\n",
    "        print('Current chrome-driver version:\\t{}'.format(chromedriver_version))\n",
    "        if major_version_different:\n",
    "            print('warning: Version different')\n",
    "            print(\n",
    "                'Download correct version at \"http://chromedriver.chromium.org/downloads\" and place in \"./chromedriver\"')\n",
    "        print('_________________________________')\n",
    "        self.browser.get('https://shopee.vn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f845e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T12:30:23.045454Z",
     "start_time": "2023-06-20T12:30:23.001941Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProductFeatureExtractor:\n",
    "    \"\"\"An object to extract products\" information from a file containing products\" URLs.\"\"\"\n",
    "\n",
    "    def __init__(self, no_gui, proxy, schemas_file_path):\n",
    "        \"\"\"Browser Initialization\n",
    "        no_gui: argument to set up a headless browser\n",
    "        proxy: argument to set up proxy server\n",
    "        schemas_file_path: file path to product schema\"\"\"\n",
    "\n",
    "        self.browser = BrowserInitializer(no_gui, proxy).browser\n",
    "        \n",
    "        self.browser.get('https://shopee.vn/buyer/login?next=https%3A%2F%2Fshopee.vn%2F')    \n",
    "        time.sleep(2)\n",
    "        self.login_shopee()\n",
    "        time.sleep(30)\n",
    "        f = open(schemas_file_path)\n",
    "        self.schema = json.load(f)\n",
    "\n",
    "    def login_shopee(self):\n",
    "        txtUser = self.browser.find_element(By.XPATH, \"//input[@type='text']\")\n",
    "        txtUser.send_keys(\"\") #username\n",
    "\n",
    "        txtPassword = self.browser.find_element(By.XPATH, \"//input[@type='password']\")\n",
    "        txtPassword.send_keys(\"\") #password\n",
    "        \n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        txtPassword.send_keys(Keys.ENTER)\n",
    "        time.sleep(3)\n",
    "\n",
    "    def extract_product_feature(self, product_url):\n",
    "        \"\"\"Method to extract all product features from a product link\n",
    "        Input:\n",
    "        product_url: link to the product page\n",
    "        schema: file path of json schema of product feature table\n",
    "        Output: A dictionary which contains all product information except for shop information\"\"\"\n",
    "\n",
    "        result = {}\n",
    "        for key, val in self.schema['properties'].items():\n",
    "            result[key] = val['default']\n",
    "\n",
    "        if not isinstance(product_url, str):\n",
    "            raise Exception(\"Invalid Product URL. Must be string type\")\n",
    "        \n",
    "        self.browser.get(product_url)\n",
    "        time.sleep(3) \n",
    "\n",
    "        self.browser.execute_script(\"window.scrollTo(0, 500);\")\n",
    "\n",
    "        total_height = int(self.browser.execute_script(\"return document.body.scrollHeight\"))\n",
    "        for i in range(1, total_height, 300):\n",
    "            self.browser.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "            time.sleep(1)\n",
    "            \n",
    "        full_page_html = self.browser.page_source\n",
    "        soup = BeautifulSoup(full_page_html, \"html.parser\")\n",
    "        html_json_application = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "        \n",
    "        # ** PRODUCT INFORMATION **\n",
    "        # extract product url\n",
    "        result[\"product_url\"] = product_url\n",
    "\n",
    "        # product name\n",
    "        result[\"name\"] = html_json_application[1].text.split('\"name\":')[1].split('\"')[1]\n",
    "\n",
    "        # brand name\n",
    "        brand_tag = html_json_application[1].text.split('\"brand\":')[1].split('\"')[1]\n",
    "        if brand_tag:\n",
    "            result[\"brand\"] = brand_tag\n",
    "        else:\n",
    "            result[\"brand\"] = np.nan\n",
    "\n",
    "        # extract industry (ngành hàng)\n",
    "        industry_tag = html_json_application[2].text.split('\"name\":')[1:-1]\n",
    "        industry_tag = [i.split('\"')[1] for i in industry_tag]\n",
    "        result[\"industry\"] = industry_tag[1]\n",
    "        result[\"product_type_lv1\"] = industry_tag[2]\n",
    "        if len(industry_tag) < 4:\n",
    "            result[\"product_type_lv2\"] = np.nan\n",
    "        else:\n",
    "            result[\"product_type_lv2\"] = industry_tag[3]\n",
    "\n",
    "        # extract description\n",
    "        result[\"description\"] = html_json_application[1].text.split('\"description\":')[1].split('\"')[1]\n",
    "\n",
    "        # extract number of image in description\n",
    "        try:\n",
    "            img_descrip = soup.find_all(\"div\", class_ = \"fS94mY\")\n",
    "            for tag in img_descrip:\n",
    "                img = tag.find(\"img\")\n",
    "                if img:\n",
    "                    result['img_description'] += 1\n",
    "        except:\n",
    "            result['img_description'] = 0\n",
    "\n",
    "        # extract product id\n",
    "        result[\"product_id\"] = html_json_application[1].text.split('\"productID\":')[1].split('\"')[1]\n",
    "\n",
    "        # extract num of variation (color, size, etc.)\n",
    "        var_1 = soup.find_all(\"button\", class_ = \"hUWqqt _69cHHm\")\n",
    "        var_2 = soup.find_all(\"button\", class_ = \"hUWqqt\")\n",
    "        num_var = len(var_1) + len(var_2)\n",
    "        result[\"num_variation\"] = num_var\n",
    "        if num_var == 0:\n",
    "            result[\"num_variation\"] = 1\n",
    "\n",
    "        # extract number of sold\n",
    "        result[\"num_sold\"] = soup.find_all(\"div\", class_=\"e9sAa2\")[0].text\n",
    "\n",
    "        # extract number of stock\n",
    "        stock_tag = soup.find_all(\"div\", string=re.compile(\"sản phẩm có sẵn|products available\"))\n",
    "        result[\"num_in_stock\"] = stock_tag[0].text.split()[0]\n",
    "\n",
    "        # extract shopee mall or not\n",
    "        header_mall = soup.find_all(\"a\", class_=\"ofs-header__page-name\")\n",
    "        mall = 0\n",
    "        if len(header_mall) != 0:\n",
    "            mall = 1\n",
    "        result[\"is_mall\"] = mall\n",
    "\n",
    "        # extract insurance information\n",
    "        insurance_tag = soup.find_all(\"section\", class_ = \"flex rY0UiC\")\n",
    "        if insurance_tag:\n",
    "            for n in insurance_tag:\n",
    "                insurance_label = n.find(\"div\", class_ =  \"flex items-center\")\n",
    "                if insurance_label is not None:\n",
    "                    result[\"insurance\"] = n.find(\"div\", class_=None).text\n",
    "        \n",
    "        try:\n",
    "            price_tag = html_json_application[1].text.split('\"price\":')[1].split('\"')[1]\n",
    "            result[\"price\"] = price_tag\n",
    "            result[\"price_min\"] = price_tag\n",
    "            result[\"price_max\"] = price_tag\n",
    "        except:\n",
    "            result['price_max'] = html_json_application[1].text.split('\"highPrice\":')[1].split('\"')[1]\n",
    "            result['price_min'] = html_json_application[1].text.split('\"lowPrice\":')[1].split('\"')[1]\n",
    "            result['price'] = result['price_min']\n",
    "\n",
    "        # extract product image - maximize 15 image\n",
    "        result_img = []\n",
    "        html_img = [i['src'] for i in soup.find_all(class_= \"_7D4JtJ\")]\n",
    "        result_img.extend(html_img)\n",
    "        if self.browser.find_elements(By.XPATH, \"//button[@class='shopee-icon-button LFMWYe _41JS8N']\"):\n",
    "            for i in range (3):\n",
    "                for i in range(5):\n",
    "                    self.browser.find_element(By.XPATH, \"//button[@class='shopee-icon-button LFMWYe _41JS8N']\").click()\n",
    "                    time.sleep(1)\n",
    "                soup = self.browser.page_source\n",
    "                soup = BeautifulSoup(soup, \"html.parser\")\n",
    "                html_img = [i['src'] for i in soup.find_all(class_= \"_7D4JtJ\")]\n",
    "                result_img.extend(html_img)\n",
    "        result_img = tuple(set(result_img))\n",
    "        result[\"num_img\"] = str(result_img)\n",
    "\n",
    "        #product rating\n",
    "        try:\n",
    "            rating = html_json_application[1].text.split('\"aggregateRating\":{\"@type\":\"AggregateRating\"',)\n",
    "            result['average_rating'] = rating[2].split('\"ratingValue\":')[1].split('\"')[1]\n",
    "        except:\n",
    "            result['average_rating'] = np.nan\n",
    "\n",
    "        #product num rating\n",
    "        try:\n",
    "            result['num_rating'] = rating[2].split('\"ratingCount\":')[1].split('\"')[1]\n",
    "        except:\n",
    "            result['num_rating'] = np.nan\n",
    "\n",
    "        # #shop rating\n",
    "        try:\n",
    "            shop_rate = rating[1].split('\"ratingValue\":')[1].split('\"')[1]\n",
    "            result['shop_rating'] = shop_rate\n",
    "        except:\n",
    "            result['shop_rating'] = np.nan\n",
    "\n",
    "        #shop num rating \n",
    "        try:\n",
    "            shop_rate_count = rating[1].split('\"ratingCount\":')[1].split('\"')[1]\n",
    "            result['shop_num_rating'] = shop_rate_count\n",
    "        except:\n",
    "            result['shop_num_rating'] = np.nan\n",
    "        \n",
    "        # extract shop url\n",
    "        shop_link_button = soup.find_all(\"div\", class_=\"Uwka-w\")\n",
    "        shop_link_tail = shop_link_button[0].find(\"a\")[\"href\"]\n",
    "        result[\"shop_url\"] = f\"https://shopee.vn{shop_link_tail}\"\n",
    "\n",
    "        # extract shop name\n",
    "        result[\"shop_name\"] = soup.find(\"div\", class_ = \"VlDReK\").text\n",
    "\n",
    "        # extract shop id\n",
    "        result['shop_id'] = html_json_application[1].text.split('\"url\":')[2].split('\"')[1].split('/')[-1]\n",
    "\n",
    "        # extract shop follower\n",
    "        shop_info = soup.find_all(\"span\", class_ = \"Xkm22X\")\n",
    "        result[\"shop_follower\"] = shop_info[0].text.split()[0]\n",
    "\n",
    "        # extract shop response rate\n",
    "        result[\"shop_response_rate\"] = shop_info[1].text.split()[0]\n",
    "\n",
    "        return result\n",
    "    def crawl_multiple_products(self, product_link_df, save_path, max_prod):\n",
    "        \"\"\"Method to crawl multiple products from a csv file containing products' URLs\n",
    "        product_link_df: DataFrame that stores products' links and names\n",
    "        save_path: file path to save output\n",
    "        max_prod: maximum number of products to crawl\n",
    "        \n",
    "        Output: a dataframe and a csv file in which each row is a product with its product features\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if max_prod >= product_link_df.shape[0]:\n",
    "            df = product_link_df\n",
    "        else:\n",
    "            df = product_link_df.loc[:max_prod]\n",
    "        url_list = df[\"product_url\"]\n",
    "        print(url_list)\n",
    "        prod_names = df[\"name\"]\n",
    "        print(prod_names)\n",
    "        output_df = pd.DataFrame(columns=list(self.schema['properties'].keys()))\n",
    "        print(output_df)\n",
    "        for url, name in zip(url_list, prod_names):\n",
    "            try:\n",
    "                try:\n",
    "                    print(\"Crawling features for product {0}\".format(name))\n",
    "                    result = self.extract_product_feature(url)\n",
    "                    print(result)\n",
    "                    output_df = output_df.append(result, ignore_index=True)\n",
    "                    print(output_df)\n",
    "                    print(f\"Execution time: {time.time() - start_time}\")\n",
    "                except KeyboardInterrupt:  # in case you lose your patience\n",
    "                    print(\"Keyboard Interrputed\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logging.error(traceback.format_exc())\n",
    "        output_df.to_csv(save_path)\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca873f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product_id\":{\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"product_url\": {\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"name\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"brand\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"industry\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"product_type_lv1\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"product_type_lv2\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"description\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"num_variation\": {\n",
    "            \"type\": 'int',\n",
    "            \"default\": 0\n",
    "        },\n",
    "        \"num_sold\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"num_in_stock\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"shop_url\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"is_mall\": {\n",
    "            \"type\": 'bool',\n",
    "            \"default\": 0\n",
    "        },\n",
    "        \"insurance\": {\n",
    "            \"type\": 'str',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"price\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"price_min\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"price_max\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"average_rating\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": 0\n",
    "        },\n",
    "        \"num_rating\": {\n",
    "            \"type\": 'float',\n",
    "            \"default\": 0\n",
    "        },\n",
    "        \"img_description\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 0\n",
    "        },\n",
    "        \"shop_id\": {\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"shop_name\": {\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"shop_follower\": {\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"shop_response_rate\": {\n",
    "            \"type\": \"str\",\n",
    "            \"default\": np.nan\n",
    "        },\n",
    "        \"num_img\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 0\n",
    "    }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b251a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if code error, create directory by yourself first\n",
    "with open(r\"..\\schemas\\product_schema.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(product_schema, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa635b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_dir(directory):\n",
    "    try:\n",
    "        os.chdir(directory)\n",
    "    except FileNotFoundError:\n",
    "        print('Invalid directory.')\n",
    "    except NotADirectoryError:\n",
    "        print('Input is a file path, not a directory.')\n",
    "    except SyntaxError:\n",
    "        print('Invalid directory syntax. Change \"\\\" to \"\\\\\" or \"/\".')\n",
    "    path = os.getcwd()\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    df_list = []\n",
    "    sheet_type = ['xlsx', 'xls', 'xlsm', 'xlsb', 'odf', 'ods']\n",
    "    for f in files:\n",
    "        file_type = f.split('.')[-1]\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(f)\n",
    "            df_list.append(df)\n",
    "            print(f'Read csv file {f}.')\n",
    "        elif file_type in sheet_type:\n",
    "            sheets = pd.ExcelFile(f).sheet_names\n",
    "            if len(sheets) > 1:\n",
    "                dfs = []\n",
    "                for s in sheets:\n",
    "                    dfs.append(pd.read_excel(f, sheet_name=s))\n",
    "                df_list.append(dfs)\n",
    "                print(f'Read {file_type} file {f} which has multiple sheets.')\n",
    "            else:\n",
    "                df_list.append(pd.read_excel(f))\n",
    "                print(f'Read {file_type} file {f} which has one sheet.')\n",
    "        else:\n",
    "            print(f'Non sheet type detected: {f}.')\n",
    "    return df_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crawler = ProductFeatureExtractor(no_gui=False, proxy=False, schemas_file_path=\"schemas/product_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb827b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"product link\\\\điện tử\"\n",
    "dfs, file_names = read_df_from_dir(directory)\n",
    "keyword = []\n",
    "for n in file_names:\n",
    "    prod_name = n.split('_')[0]\n",
    "    keyword.append(prod_name)\n",
    "    \n",
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886066ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for df, kw in zip(dfs, keyword):\n",
    "    print(\"\\n\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"{0}. Crawling products for {1}...\".format(count, kw))\n",
    "    # print(\"-----------------------------------------------------------\")\n",
    "    product_crawler.crawl_multiple_products(product_link_df=df,\n",
    "                                            save_path=r\"C:\\Users\\LENOVO\\OneDrive - National Economics University\\Manh and DSEB\\DSLab\\Mapping & Evaluate Ecommerce Site\\shopee_crawl\\product features\\{0}.csv\".format(kw), max_prod=400)\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
